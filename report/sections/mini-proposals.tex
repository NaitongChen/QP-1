% !TEX root = ../main.tex

% Mini-proposals section

% each mini-proposal gets its own subsection
\section{Mini-proposals}

\subsection{Proposal 1: Using SURE to automate cross validation for principal component regression with L1 regularization} % enter your proposal title
\subsubsection{Motivation and problem formulation}
Principal component regression (PCR), a regression method based on principal components (PCs) obtained from principal component analyses (PCA), is commonly used in genomics applications among others. This is because, rather than the effect of individual gene expressions, practitioners are typically more interested in studying the effect of groups of genes, which may describe more complicated processes \citep{ding2022sufficient}. 

$ $\newline
The general procedure of PCR is typically as follows. Suppose we are given a centred data matrix $X \in \reals^{n\times p}$ and corresponding responses $y\in\reals^n$ consisting of $n$ observations each with $p$ predictors. Let $X = U\Sigma V^T$ denote the singular value decomposition of the data matrix $X$, then $W = XV$ denotes the set of all PCs, a set of transformed feature vectors that form an orthonormal basis, ordered by the amount of variance from the data that each PC explains. One can then perform linear regression on the PCs against the response $y$. Because the PCs are orthogonal to each other, this approach usually leads to better numerical stability than, for example, ordinary least squares. 

$ $\newline
When the total number of PCs is large, a common practice is to perform PCR using only the top $k$ PCs ($k\ll p$), i.e., the PCs that explain the most variance present in the data \citep{cera2019genes,harel2019predicting}. However, it is important to note that the PCs are fit without knowledge of the response variable, and so the idea that performing PRC using the first $k$ PCs will lead to good a good fit of the data is merely a heuristic. In fact, \citet{jolliffe1982note} provides a number of real examples where performing PCR using the first few PCs that explain the most variance of the data is indeed suboptimal.

$ $\newline
As a result, it is desirable to do feature selection among the PCs so that we can balance the number of active predictors and the quality of our PCR model. From the previous section, we know that the SURE can be used as a model selection tool for selecting the regularization parameter for ridge regression. It is then natural to look into extending this framework to LASSO regression on the PCs, which can help us pick the PCs to include in the regression while taking into account the bias-variance trade-off.

\subsubsection{Proposed approach}
Given the transformed data matrix $W$, a response vector $y$ consisting of $n$ observations, and a regularization parameter $\lambda\geq0$, LASSO regression finds the regression coefficients of the form
\[
\hbeta_{L,\lambda}(y) = \argmin_{\beta} \frac{1}{n}\|W\beta - y\|_2^2 + \lambda\|\beta\|_1.
\]
This L1 penalty term encourages sparse solutions where the regression coefficient for some the transformed predictors are set to $0$. To use the SURE framework to select a subset of the PCs, we require that 1) the resulting estimator $\hbeta_{L,\lambda}$ to be available in closed-form, and 2) we can compute the derivative of this estimator with respect to $y$.

$ $\newline
In the context of PCR, we know that $W$ is an orthonormal matrix. We know that when the data matrix is orthonormal, there is indeed a closed-form solution for the LASSO estimator
\[
\hbeta_{L,\lambda} = \begin{bmatrix} \sgn(\hbeta_1)(|\hbeta_1| - \lambda)_+ & \cdots & \sgn(\hbeta_p)(|\hbeta_p| - \lambda)_+ \end{bmatrix}^T,
\]
where $\hbeta$ denotes the solution of the ordinary least square problem \citep{gauraha2018introduction}. Note that $\hbeta_{L,\lambda}$ is implicitly a function of the response $y$. Furthermore, \citet{tibshirani2015stein} provides a derivation that shows the divergence term in the SURE expression equals the number of predictors whose corresponding regression coefficient is nonzero (i.e. $\|\hbeta_{L,\lambda}\|_0$). As a result, we can write the SURE of the LASSO estimator on a set of PCs as 
\[
\hat{R}(\lambda) = -n\sigma^2 + \| y - \hbeta_{L,\lambda}(y) \|^2 + 2\sigma^2\|\hbeta_{L,\lambda}\|_0.
\]
It now remains to find the optimal $\lambda$ and subsequently the set of PCs with nonzero regression coefficients. Since each dimension of the LASSO estimator, denoted $\hbeta_{L,\lambda,i}$ is non-smooth at $\hbeta_{L,\lambda,i} = \lambda$, we can use subgradient methods in place of regular gradient descent to obtain the optimal $\lambda$ \citep{shor2012minimization}. Instead of selecting the top $k$ PCs to perform PCR, where $k$ is chosen somewhat arbitrarily, an SURE-inspired variable selection procedure proposed here may help achieve a better quality PCR fit with a similar level of reduction in computational cost.

\pagebreak

\subsection{Proposal 2: Efficient cross-validation for ridge regression via data subsampling} % enter your proposal title
\subsubsection{Motivation and problem formulation}
Following the notation from the previous sections, given a data matrix $X\in\reals^{n\times d}$, a response vector $y\in\reals^n$, and a regularization parameter $\lambda\geq0$, the ridge estimator of the regression coefficients takes the form
\[
\hbeta_{\text{ridge}, \lambda}(y) = \left( X^TX + \lambda I \right)^{-1}X^Ty,
\]
which has a computational complexity of $O(np^2)$. In the context of model selection by minimizng the SURE, one way to obtain the optimal regularization parameter is to automatic differentiation and run the gradient descent algorithm with respect to $\lambda$ with the SURE being the objective function. This requires, at each iteration of the optimization, evaluating the derivative of SURE with respect to $\lambda$, which involves the term $\hbeta_{\text{ridge}, \lambda}(y)$. Therefore, for $K$ steps of gradient descent, the computational complexity is at least in the order of $O(Knp^2)$. In the large-data regime where there are many observations, the cost of this procedure grows linearly with the number of optimization iterations, which can potentially be extremely expensive. Similarly, in the perhaps more commonly used $K$-fold cross-validation procedure, we need to compute $\hbeta_{\text{ridge}, \lambda}$ for each of the $K$ folds of the data. Then if $K$ is also large, the computational cost could also be expensive.

$ $\newline
To reduce the computational complexity of such procedures, one approach is to use a representative subsample of size $m$ ($m\ll n$) in place of the full datase to perform model selection. If the subsample is representative of the full dataset, we can reduce the computational cost without greatly hindering the quality of the selected model. Existing approaches for subsample selection include leverage score based and volume sampling based methods. More specifically, one can either use the leverage scores to form an importance distribution from which we sample the observations, or use the idea of volume sampling to select a subsample that minimizes the Frobenius norm between the original data matrix and the projection onto the space spanned by the selected subset \citep{ma2014statistical,avron2010blendenpik}.

$ $\newline
It is worth noting that these methods mentioned above do not take into consideration the subsequet model selection step while constructing the subset. While there has been some work that concerns the generalization error of the ridge estimator based on a subset of the data, their result only holds for $\lambda$ values that are bounded by some function of the true regression coefficients, which are not known a priori \citep{derezinski2017subsampling}.

$ $\newline
In the context of model selection for ridge regression, ideally we would like a subsample of the data that well approximates the original dataset for a wide range of $\lambda$ values, or at least the likely $\lambda$ values given the full dataset. From a probabilistic point of view, we know that the ridge estimator is the MAP estimator with Gaussian likelihood for each observation (with mean $x_i^T\beta$ and variance $\sigma^2$) and Gaussian prior on the regression parameters (with the mean of each dimension being $0$ and variance $\tau^2$):
\[
\hbeta_{\text{ridge},\lambda} = \argmax_{\beta} -\frac{1}{2}\sum_{i=1}^n (x_i^T\beta - y_i)^2 - \frac{\sigma^2 / \tau^2}{2}\|\beta\|_2^2,
\]
where we can set $\lambda = \frac{\sigma^2}{\tau^2}$. From a Bayesian perspective, the Gaussian likelihood on the data and Gaussian prior on the regression parameters together give us a posterior distribution of the regression parameters upon observing the data. Note that the above formulation is still specific to a particular $\lambda$ value. Upon imposing another prior distribution over $\tau$, we can obtain a posterior distribution $\pi$ over $\tau$ and $\beta$, and subsequently $\lambda$. If we can build a sparse and potentially weighted subset (of size $m$) of the full dataset that has low error across the high density regions of $\pi$, it is possible to obtain good generalization properties of the ridge estimator resulted from either the SURE or cross-validation model selection procedure.

\subsubsection{Proposed approach}
One way to formulate this idea is as follows. Given an approximation of the posterior distribution over $\lambda$ and $\beta$, which we denote $\hpi$, we would like to find a set of weights $w \in\reals_+^n$, $\|w\|_0 \leq m$, that minimizes
\[
\left\| \sum_{i=1}^n \mcL_i - \sum_{i=1}^n w_i \mcL_i \right\|_{\hpi, 2}^2,
\]
where $\mcL_i$ is the Gaussian log likelihood for the $i^\text{th}$ observation, and $\|\cdot\|_{\hpi, 2}$ denotes a $\hpi$-weighted $L^2$ norm on the log likelihoods. It turns out that this is precisely the objective for one of the Bayesian coreset construction algorithms, where we can construct a sparse, weighted subsample of the original dataset \citep{campbell2019automated}. Since we know that there exists closed-form solutions for weighted least squares, we can use this Bayesian coreset to construct our ridge regression models and to subsequently perform model selection either through SURE or cross-validation \citep{strutz2011data}.

$ $\newline
In \citet{campbell2019automated}, a bound on the above objective under the set of optimal weights obtained using random projection \citep{rahimi2007random} and samples from $\hpi$ is provided. In particular, this bound is a function of $m$ that decreases as $m$ increases. Given the connection between the ridge estimator and its underlying posterior distribution, we should expect to see good generalization properties of the selected model constructed using Bayesian coresets.

$ $\newline
In order to construct Bayesian coresets through the above formulation, we need to be able to take samples from $\hpi$. We note that ridge regression corresponds to a posterior distribution with a Gaussian likelihood on the data and a Gaussian prior on the regression coefficients. Therefore, with an appropriate choice of the prior on $\tau$, it should not be difficult to obtain a $\hpi$ that well approximates the true posterior. One option to construct $\hpi$ is to use the Laplace approximaiton \citep{bishop2006pattern}. It remains to explore the effect that different choices of the prior on $\lambda$ has on the quality of the selected ridge regression model, either through SURE or cross-validation.

% each mini-proposal gets its own subsection
% ...